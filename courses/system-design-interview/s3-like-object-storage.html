<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>S3-like Object Storage - System Design Interview</title>
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;700&display=swap" rel="stylesheet">
    <style>* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: 'Open Sans', Arial, sans-serif;
    font-size: 16px;
    line-height: 1.6;
    color: #454545;
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
}

h1 {
    font-size: 2em;
    margin: 1em 0 0.5em 0;
    line-height: 1.2;
}

h2 {
    font-size: 1.5em;
    margin: 1.2em 0 0.6em 0;
    line-height: 1.3;
}

h3 {
    font-size: 1.2em;
    margin: 1em 0 0.5em 0;
    line-height: 1.4;
}

p {
    margin: 0.8em 0;
}

a {
    color: #0077aa;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}

ul, ol {
    margin: 0.8em 0;
    padding-left: 2em;
}

li {
    margin: 0.4em 0;
}

code, pre {
    background: #f4f4f4;
    border: 1px solid #ddd;
    border-radius: 3px;
    font-family: 'Courier New', monospace;
    font-size: 0.9em;
}

code {
    padding: 2px 5px;
}

pre {
    padding: 10px;
    overflow-x: auto;
    margin: 1em 0;
}

pre code {
    border: none;
    padding: 0;
}

img {
    max-width: 100%;
    height: auto;
    display: block;
    margin: 1.5em auto;
}

figure {
    margin: 1.5em 0;
    text-align: center;
}

figcaption {
    font-style: italic;
    font-size: 0.9em;
    color: #666;
    margin-top: 0.5em;
}

.chapter-number {
    font-weight: bold;
    color: #0077aa;
}

.nav {
    margin: 2em 0;
    padding: 1em 0;
    border-top: 1px solid #ddd;
    border-bottom: 1px solid #ddd;
}

.nav a {
    margin-right: 1em;
}

.toc {
    list-style: none;
    padding: 0;
}

.toc li {
    margin: 0.8em 0;
}

.course-section {
    margin: 2em 0;
}

.course-title {
    font-size: 1.3em;
    font-weight: bold;
    color: #0077aa;
    margin: 1.5em 0 0.8em 0;
}

.metadata {
    font-size: 0.9em;
    color: #888;
    margin: 2em 0 1em 0;
}

.breadcrumb {
    font-size: 0.9em;
    color: #666;
    margin-bottom: 1em;
}

.breadcrumb a {
    color: #0077aa;
}

@media (max-width: 600px) {
    body {
        padding: 10px;
    }

    h1 {
        font-size: 1.5em;
    }

    h2 {
        font-size: 1.3em;
    }
}</style>
</head>
<body>
    <div class="breadcrumb">
        <a href="../../index.html">Home</a> /
        <a href="../system-design-interview.html">System Design Interview</a> /
        S3-like Object Storage
    </div>

    <div class="nav">
        <a href="../system-design-interview.html">← Course Contents</a>
        <a href="distributed-email-service.html">← Previous</a>
        <a href="real-time-gaming-leaderboard.html">Next →</a>
    </div>

    <main>
        <h1>
            <span class="chapter-number">25</span> S3-like Object Storage
        </h1>

        <div class="metadata">
            <a href="https://bytebytego.com/courses/system-design-interview/s3-like-object-storage" target="_blank" rel="noopener">Original source</a>
        </div>

        <article>
            <header><strong class="style_chapter__grtAe">25</strong><h1>S3-like Object Storage</h1></header><p>In this chapter, we design an object storage service similar to Amazon Simple Storage Service (S3). S3 is a service offered by Amazon Web Services (AWS) that provides object storage through a RESTful API-based interface. Here are some facts about AWS S3:</p>
<ul>
<li>
<p>Launched in June 2006.</p>
</li>
<li>
<p>S3 added versioning, bucket policy, and multipart upload support in 2010.</p>
</li>
<li>
<p>S3 added server-side encryption, multi-object delete, and object expiration in 2011.</p>
</li>
<li>
<p>Amazon reported 2 trillion objects stored in S3 by 2013.</p>
</li>
<li>
<p>Life cycle policy, event notification, and cross-region replication support were introduced in 2014 and 2015.</p>
</li>
<li>
<p>Amazon reported over 100 trillion objects stored in S3 by 2021.</p>
</li>
</ul>
<p>Before we dig into object storage, let’s first review storage systems in general and define some terminologies.</p>
<h2 id="storage-system-101">Storage System 101</h2>
<p>At a high-level, storage systems fall into three broad categories:</p>
<ul>
<li>
<p>Block storage</p>
</li>
<li>
<p>File storage</p>
</li>
<li>
<p>Object storage</p>
</li>
</ul>
<p><strong>Block storage</strong></p>
<p>Block storage came first, in the 1960s. Common storage devices like hard disk drives (HDD) and solid-state drives (SSD) that are physically attached to servers are all considered as block storage.</p>
<p>Block storage presents the raw blocks to the server as a volume. This is the most flexible and versatile form of storage. The server can format the raw blocks and use them as a file system, or it can hand control of those blocks to an application. Some applications like a database or a virtual machine engine manage these blocks directly in order to squeeze every drop of performance out of them.</p>
<p>Block storage is not limited to physically attached storage. Block storage could be connected to a server over a high-speed network or over industry-standard connectivity protocols like Fibre Channel (FC) [1] and iSCSI [2]. Conceptually, the network-attached block storage still presents raw blocks. To the servers, it works the same as physically attached block storage.</p>
<p><strong>File storage</strong></p>
<p>File storage is built on top of block storage. It provides a higher-level abstraction to make it easier to handle files and directories. Data is stored as files under a hierarchical directory structure. File storage is the most common general-purpose storage solution. File storage could be made accessible by a large number of servers using common file-level network protocols like SMB/CIFS [3] and NFS [4]. The servers accessing file storage do not need to deal with the complexity of managing the blocks, formatting volume, etc. The simplicity of file storage makes it a great solution for sharing a large number of files and folders within an organization.</p>
<p><strong>Object storage</strong></p>
<p>Object storage is new. It makes a very deliberate tradeoff to sacrifice performance for high durability, vast scale, and low cost. It targets relatively “cold” data and is mainly used for archival and backup. Object storage stores all data as objects in a flat structure. There is no hierarchical directory structure. Data access is normally provided via a RESTful API. It is relatively slow compared to other storage types. Most public cloud service providers have an object storage offering, such as AWS S3, Google object storage, and Azure blob storage.</p>
<p><strong>Comparison</strong></p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents three different storage models: Block Storage, File Storage, and Object Storage.  The Block Storage model is depicted as a cylindrical database containing nine equally sized blocks, each labeled 'block,' representing the fundamental units of storage.  The File Storage model shows a hierarchical structure where a root folder branches into multiple subfolders and individual files, represented by folder and file icons respectively. Arrows indicate the relationships between folders and files, showing a parent-child relationship.  Finally, the Object Storage model illustrates a graph-like structure where each 'Object' is connected to one or more 'Payload' nodes representing the data and one or more 'Meta data' nodes containing metadata about the object.  Each object is also uniquely identified by an 'ID' node.  The three models are presented side-by-side for comparison, highlighting the different ways data can be organized and accessed in a storage system." loading="lazy" width="750" height="267" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-1-three-different-storage-options-WEC7EEGI.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 1 Three different storage options</figcaption></div></figure>
<p>Table 1 compares block storage, file storage, and object storage.</p>
<div class="table-wrap" style="--table-min-width: 640px;"><table><thead><tr><td><strong>Block storage</strong></td><td><strong>File storage</strong></td><td><strong>Object storage</strong></td><td></td></tr></thead><tbody><tr><td>Mutable Content</td><td>Y</td><td>Y</td><td>N (object versioning is supported, in-place update is not）</td></tr><tr><td>Cost</td><td>High</td><td>Medium to high</td><td>Low</td></tr><tr><td>Performance</td><td>Medium to high, very high</td><td>Medium to high</td><td>Low to medium</td></tr><tr><td>Consistency</td><td>Strong consistency</td><td>Strong consistency</td><td>Strong consistency [5]</td></tr><tr><td>Data access</td><td>SAS [6]/iSCSI/FC</td><td>Standard file access, CIFS/SMB, and NFS</td><td>RESTful API</td></tr><tr><td>Scalability</td><td>Medium scalability</td><td>High scalability</td><td>Vast scalability</td></tr><tr><td>Good for</td><td><p>Virtual machines (VM), high-performance applications like database</p></td><td>General-purpose file system access</td><td>Binary data, unstructured data</td></tr></tbody></table></div>
<p>Table 1 Storage options</p>
<h3 id="terminology">Terminology</h3>
<p>To design S3-like object storage, we need to understand some core object storage concepts first. This section provides an overview of the terms that apply to object storage.</p>
<p><strong>Bucket</strong>. A logical container for objects. The bucket name is globally unique. To upload data to S3, we must first create a bucket.</p>
<p><strong>Object</strong>. An object is an individual piece of data we store in a bucket. It contains object data (also called payload) and metadata. Object data can be any sequence of bytes we want to store. The metadata is a set of name-value pairs that describe the object.</p>
<p><strong>Versioning</strong>. A feature that keeps multiple variants of an object in the same bucket. It is enabled at bucket-level. This feature enables users to recover objects that are deleted or overwritten by accident.</p>
<p><strong>Uniform Resource Identifier (URI)</strong>. The object storage provides RESTful APIs to access its resources, namely, buckets and objects. Each resource is uniquely identified by its URI.</p>
<p><strong>Service-level agreement (SLA)</strong>. A service-level agreement is a contract between a service provider and a client. For example, the Amazon S3 Standard-Infrequent Access storage class provides the following SLA [7]:</p>
<ul>
<li>
<p>Designed for durability of 99.999999999% of objects across multiple Availability Zones.</p>
</li>
<li>
<p>Data is resilient in the event of one entire Availability Zone destruction.</p>
</li>
<li>
<p>Designed for 99.9% availability.</p>
</li>
</ul>
<h2 id="step-1---understand-the-problem-and-establish-design-scope">Step 1 - Understand the Problem and Establish Design Scope</h2>
<p>The following questions help to clarify the requirements and narrow down the scope.</p>
<p><strong>Candidate</strong>: Which features should be included in the design?<br>
<strong>Interviewer</strong>: We would like you to design an S3-like object storage system with the following functionalities:</p>
<ul>
<li>
<p>Bucket creation.</p>
</li>
<li>
<p>Object uploading and downloading.</p>
</li>
<li>
<p>Object versioning.</p>
</li>
<li>
<p>Listing objects in a bucket. It’s similar to the “aws s3 ls” command [8].</p>
</li>
</ul>
<p><strong>Candidate</strong>: What is the typical data size?<br>
<strong>Interviewer</strong>: We need to store both massive objects (a few GBs or more) and a large number of small objects (tens of KBs,) efficiently.</p>

<p><strong>Candidate</strong>: How much data do we need to store in one year?<br>
<strong>Interviewer</strong>: 100 petabytes (PB).</p>
<p><strong>Candidate</strong>: Can we assume data durability is 6 nines (99.9999%) and service availability is 4 nines (99.99%)?<br>
<strong>Interviewer</strong>: Yes, that sounds reasonable.</p>
<h3 id="non-functional-requirements">Non-functional requirements</h3>
<ul>
<li>
<p>100 PB of data</p>
</li>
<li>
<p>Data durability is 6 nines</p>
</li>
<li>
<p>Service availability is 4 nines</p>
</li>
<li>
<p>Storage efficiency. Reduce storage costs while maintaining a high degree of reliability and performance.</p>
</li>
</ul>
<h3 id="back-of-the-envelope-estimation">Back-of-the-envelope estimation</h3>
<p>Object storage is likely to have bottlenecks in either disk capacity or disk IO per second (IOPS). Let’s take a look.</p>
<ul>
<li>
<p>Disk capacity. Let’s assume objects follow the distribution listed below:</p>
<ul>
<li>
<p>20% of all objects are small objects (less than 1MB).</p>
</li>
<li>
<p>60% of objects are medium-sized objects (1MB ~ 64MB).</p>
</li>
<li>
<p>20% are large objects (larger than 64MB).</p>
</li>
</ul>
</li>
<li>
<p>IOPS. Let’s assume one hard disk (SATA interface, 7200 rpm) is capable of doing 100~150 random seeks per second (100-150 IOPS).</p>
</li>
</ul>
<p>With those assumptions, we can estimate the total number of objects the system can persist. To simplify the calculation, let’s use the median size for each object type (0.5MB for small objects, 32MB for medium objects, and 200MB for large objects). A 40% storage usage ratio gives us:</p>
<ul>
<li>
<p>100 PB = 100*1000*1000*1000 MB = 10^11 MB</p>
</li>
<li>
<p>10^11*0.4/( 0.2*0.5MB + 0.6 *32MB + 0.2*200MB) = 0.68 billion objects.</p>
</li>
<li>
<p>If we assume the metadata of an object is about 1KB in size, we need 0.68 TB space to store all metadata information.</p>
</li>
</ul>
<p>Even though we may not use those numbers, it’s good to have a general idea about the scale and constraint of the system.</p>
<h2 id="step-2---propose-high-level-design-and-get-buy-in">Step 2 - Propose High-Level Design and Get Buy-In</h2>
<p>Before diving into the design, let’s explore a few interesting properties of object storage, as they may influence it.</p>
<p><strong>Object immutability</strong>. One of the main differences between object storage and the other two types of storage systems is that the objects stored inside of object storage are immutable. We may delete them or replace them entirely with a new version, but we cannot make incremental changes.</p>
<p><strong>Key-value store</strong>. We could use object URI to retrieve object data (Listing 1). The object URI is the key and object data is the value.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><pre><code>Request:
GET /bucket1/object1.txt HTTP/1.1

Response:
HTTP/1.1 200 OK
Content-Length: 4567

[4567 bytes of object data]
</code></pre></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Listing 1 Use object URI to retrieve object data</figcaption></div></figure>
<p><strong>Write once, read many times</strong>. The data access pattern for object data is written once and read many times. According to the research done by LinkedIn, 95% of requests are read operations [9].</p>
<p><strong>Support both small and large objects</strong>. Object size may vary and we need to support both.</p>
<p>The design philosophy of object storage is very similar to that of the UNIX file system. In UNIX, when we save a file in the local file system, it does not save the filename and file data together. Instead, the filename is stored in a data structure called “inode” [10], and the file data is stored in different disk locations. The inode contains a list of file block pointers that point to the disk locations of the file data. When we access a local file, we first fetch the metadata in the inode. We then read the file data by following the file block pointers to the actual disk locations.</p>
<p>The object storage works similarly. The inode becomes the metadata store that stores all the object metadata. The hard disk becomes the data store that stores the object data. In the UNIX file system, the inode uses the file block pointer to record the location of data on the hard disk. In object storage, the metadata store uses the ID of the object to find the corresponding object data in the data store, via a network request. Figure 2 shows the UNIX file system and the object storage.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a comparison of Unix File System and Object Store System architectures.  The left side depicts the Unix File System, showing an `inode` containing metadata like `File Name`, `Owner UID`, `Group UID`, `Mode`, and a set of `File block pointers`. These pointers indicate the location of the actual file data on the `Hard disk` via `Local Disk Access`.  The hard disk is represented as a circle containing several smaller rectangles symbolizing individual file blocks.  On the right, the Object Store System is shown, divided into `MetaStore` and `DataStore`. The `MetaStore` is a key-value store mapping `Object name` to `Object ID`.  A `Network Request` targets the `DataStore`, which contains the actual object data.  The request flows from the `MetaStore` to the `DataStore` via an `ID`, retrieved from the `MetaStore` based on the object name.  The `DataStore` is represented by a circle labeled 'Object' connected to a circle labeled 'ID', illustrating the retrieval of the object using its ID." loading="lazy" width="500" height="400" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-2-unix-file-system-and-object-store-DCHSEP2D.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 2 Unix file system and object store</figcaption></div></figure>
<p>Separating metadata and object data simplifies the design. The data store contains immutable data while the metadata store contains mutable data. This separation enables us to implement and optimize these two components independently. Figure 3 shows what the bucket and object look like.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a simplified schema of an object storage system.  On the left, a rectangular box labeled 'Metadata' contains fields for 'ID,' 'Bucket Name,' 'Policy,' 'Life Cycle,' and an ellipsis indicating more fields.  A line connects this metadata box to a filled-in glass icon representing a 'Bucket.'  To the right, a circle represents an 'Object,' connected by lines to two rectangular boxes. One box, labeled 'Data,' contains three lines of seemingly random binary data, representing the actual stored object data. The other box, labeled 'Metadata,' contains fields for 'ID,' 'Object Name,' 'Version ID,' 'Expiration,' 'Access Control,' and 'Bucket,' providing metadata about the object.  The connections show that each object is associated with both its data and its metadata, and that buckets have associated metadata." loading="lazy" width="500" height="237" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-3-bucket-and-object-A4GTINXY.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 3 Bucket &amp; object</figcaption></div></figure>
<h3 id="high-level-design">High-level design</h3>
<p>Figure 4 shows the high-level design.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a system architecture diagram showing the flow of data and interactions between different components.  A user initiates a request, which is first handled by a load balancer distributing the traffic across multiple instances (implied by the three boxes under the load balancer). The request then proceeds to an API Service, which interacts with an Identity &amp; Access Management system for authentication and authorization.  The API Service subsequently communicates with a Metadata Service, which in turn interacts with a Metadata DB (Metadata Store).  Concurrently, the API Service also interacts with a Data Store, which is composed of three services: a primary Data Store Service and two secondary Data Store Services. Each Data Store Service connects to a separate Storage Node, creating a primary and two secondary storage nodes for redundancy.  The arrows indicate the direction of data flow between these components." loading="lazy" width="600" height="415" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-4-high-level-design-JIVBP5X2.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 4 High-level design</figcaption></div></figure>
<p>Let’s go over the components one by one.</p>
<p><strong>Load balancer</strong>. Distributes RESTful API requests across a number of API servers.</p>
<p><strong>API service</strong>. Orchestrates remote procedure calls to the identity and access management service, metadata service, and storage stores. This service is stateless so it can be horizontally scaled.</p>
<p><strong>Identity and access management (IAM)</strong>. The central place to handle authentication, authorization, and access control. Authentication verifies who you are, and authorization validates what operations you could perform based on who you are.</p>
<p><strong>Data store</strong>. Stores and retrieves the actual data. All data-related operations are based on object ID (UUID).</p>
<p><strong>Metadata store</strong>. Stores the metadata of the objects.</p>
<p>Note that the metadata and data stores are just logical components, and there are different ways to implement them. For example, in Ceph’s Rados Gateway [11], there is no standalone metadata store. Everything, including the object bucket, is persisted as one or multiple Rados objects.</p>
<p>Now we have a basic understanding of the high-level design, let’s explore some of the most important workflows in object storage.</p>
<ul>
<li>
<p>Uploading an object.</p>
</li>
<li>
<p>Downloading an object.</p>
</li>
<li>
<p>Object versioning and listing objects in a bucket. They will be explained in the “Deep Dive” section.</p>
</li>
</ul>
<h3 id="uploading-an-object">Uploading an object</h3>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a system architecture diagram for object storage.  A user initiates the process by sending an HTTP PUT request (①) to create a bucket or (④) to create an object (script.txt). These requests are routed through a load balancer to an API Service.  Before processing, the API Service interacts with an Identity &amp; Access Management system (②, ⑤) for user authentication and authorization.  The API Service then creates metadata for the bucket (③) or object (⑦), interacting with a Metadata Service which persists this information in a Metadata DB (Metadata Store).  The actual object upload (⑥) is handled by the API Service, which sends the object to a Data Service. This Data Service replicates the data across a primary and two secondary Storage Nodes, forming a Data Store with high availability.  The numbered annotations (①-⑦) indicate the sequential flow of operations within the system." loading="lazy" width="600" height="418" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-5-uploading-an-object-ASOV6ZCY.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 5 Uploading an object</figcaption></div></figure>
<p>An object has to reside in a bucket. In this example, we first create a bucket named “bucket-to-share” and then upload a file named “script.txt” to the bucket. Figure 5 explains how this flow works in 7 steps.</p>
<ol>
<li>
<p>The client sends an HTTP PUT request to create a bucket named “bucket-to-share.” The request is forwarded to the API service.</p>
</li>
<li>
<p>The API service calls the IAM to ensure the user is authorized and has WRITE permission.</p>
</li>
<li>
<p>The API service calls the metadata store to create an entry with the bucket info in the metadata database. Once the entry is created, a success message is returned to the client.</p>
</li>
<li>
<p>After the bucket is created, the client sends an HTTP PUT request to create an object named “script.txt”.</p>
</li>
<li>
<p>The API service verifies the user’s identity and ensures the user has WRITE permission on the bucket.</p>
</li>
<li>
<p>Once validation succeeds, the API service sends the object data in the HTTP PUT payload to the data store. The data store persists the payload as an object and returns the UUID of the object.</p>
</li>
<li>
<p>The API service calls the metadata store to create a new entry in the metadata database. It contains important metadata such as the object_id (UUID), bucket_id (which bucket the object belongs to), object_name, etc. A sample entry is shown in Table 2.</p>
</li>
</ol>
<div class="table-wrap" style="--table-min-width: 500px;"><table><thead><tr><td><strong>object_name</strong></td><td><strong>object_id</strong></td><td><strong>bucket_id</strong></td></tr></thead><tbody><tr><td>script.txt</td><td>239D5866-0052-00F6-014E-C914E61ED42B</td><td>82AA1B2E-F599-4590-B5E4-1F51AAE5F7E4</td></tr></tbody></table></div>
<p>Table 2 Sample entry</p>
<p>The API to upload an object could look like this:</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><pre><code>PUT /bucket-to-share/script.txt HTTP/1.1
Host: foo.s3example.org
Date: Sun, 12 Sept 2021 17:51:00 GMT
Authorization: authorization string
Content-Type: text/plain
Content-Length: 4567
x-amz-meta-author: Alex

[4567 bytes of object data]
</code></pre></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Listing 2 Uploading an object</figcaption></div></figure>
<h3 id="downloading-an-object">Downloading an object</h3>
<p>A bucket has no directory hierarchy. However, we can create a logical hierarchy by concatenating the bucket name and the object name to simulate a folder structure. For example, we name the object “bucket-to-share/script.txt” instead of “script.txt”. To get an object, we specify the object name in the GET request. The API to download an object looks like this:</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><pre><code>GET /bucket-to-share/script.txt HTTP/1.1
Host: foo.s3example.org
Date: Sun, 12 Sept 2021 18:30:01 GMT
Authorization: authorization string
</code></pre></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Listing 3 Downloading an object</figcaption></div></figure>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a system architecture diagram for object storage and retrieval.  A user initiates a request (HTTP GET: `/bucket-to-share/script.txt`) which is first handled by a load balancer distributing the traffic across multiple API Service instances.  The request then proceeds to the API Service, where step ② (Identity validation and authorization) occurs via interaction with an Identity &amp; Access Management system.  Step ③ involves querying a Metadata DB (Metadata store) via the Metadata Service to obtain the object's location (UUID).  This UUID is then used (step ④) by the API Service to retrieve the object from the Data Store, which consists of three Storage Service instances: one primary and two secondary.  The primary Storage Service holds the main copy of the object, with the secondary services acting as replicas.  Finally, step ⑤ (Download object) delivers the retrieved object back to the user.  The entire flow is sequential, with each step dependent on the successful completion of the previous one." loading="lazy" width="600" height="429" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-6-downloading-an-object-V3F76264.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 6 Downloading an object</figcaption></div></figure>
<p>As mentioned earlier, the data store does not store the name of the object and it only supports object operations via object_id (UUID). In order to download the object, we first map the object name to the UUID. The workflow of downloading an object is shown below:</p>
<ol>
<li>
<p>The client sends an HTTP GET request to the load balancer: GET /bucket-to-share/script.txt</p>
</li>
<li>
<p>The API service queries the IAM to verify that the user has READ access to the bucket.</p>
</li>
<li>
<p>Once validated, the API service fetches the corresponding object’s UUID from the metadata store.</p>
</li>
<li>
<p>Next, the API service fetches the object data from the data store by its UUID.</p>
</li>
<li>
<p>The API service returns the object data to the client in HTTP GET response.</p>
</li>
</ol>
<h2 id="step-3---design-deep-dive">Step 3 - Design Deep Dive</h2>
<p>In this section, we dive deep into a few areas:</p>
<ul>
<li>
<p>Data store</p>
</li>
<li>
<p>Metadata data model</p>
</li>
<li>
<p>Listing objects in a bucket</p>
</li>
<li>
<p>Object versioning</p>
</li>
<li>
<p>Optimizing uploads of large files</p>
</li>
<li>
<p>Garbage collection</p>
</li>
</ul>
<h3 id="data-store">Data store</h3>
<p>Let’s take a closer look at the design of the data store. As discussed previously, the API service handles external requests from users and calls different internal services to fulfill those requests. To persist or retrieve an object, the API service calls the data store. Figure 7 shows the interactions between the API service and the data store for uploading and downloading an object.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a simplified system architecture diagram illustrating object upload and download functionalities.  The diagram is divided into two sections: 'Upload Object' and 'Download Object'. Each section shows an 'API Service' box interacting with a 'DataStore' cylinder. In the 'Upload Object' section, an arrow labeled 'Request: Upload' and 'Payload: file_content' points from the 'API Service' to the 'DataStore', indicating the upload process.  A response arrow points back to the 'API Service' labeled 'Response: ObjectID=30a3e98e-55d9-11ec-bf63-0242ac130002', showing the unique identifier assigned to the uploaded object.  The 'Download Object' section mirrors this, but with an arrow labeled 'Request: Download' and 'ObjectID: 30a3e98e-55d9-11ec-bf63-0242ac130002' pointing from the 'API Service' to the 'DataStore', specifying the object to retrieve.  The response arrow back to the 'API Service' is labeled 'Response: data = file_content', indicating the downloaded data.  Both sections depict a client-server interaction where the API Service acts as an intermediary between a client (implied) and the DataStore, managing object storage and retrieval." loading="lazy" width="600" height="311" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-7-upload-and-download-an-object-YPKTXFPC.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 7 Upload and download an object</figcaption></div></figure>
<h4 id="high-level-design-for-the-data-store">High-level design for the data store</h4>
<p>The data store has three main components as shown in Figure 8.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a simplified architecture diagram illustrating data replication and routing.  A central 'Placement Service' component receives 'Data traffic' from a 'Data Routing Service.'  The Placement Service then sends this data to three separate storage nodes, each represented by a hard drive icon.  The connections between the Placement Service and each storage node are labeled 'Heartbeat,' indicating a regular communication for health checks and coordination.  Each storage node also receives 'Data replication' from the Placement Service, ensuring data redundancy.  The overall flow shows data moving from the routing service, through the placement service, and finally being replicated across multiple storage nodes for high availability and fault tolerance." loading="lazy" width="600" height="349" decoding="async" data-nimg="1" sizes="(max-width: 840px) min(600px, 100vw), (max-width: 1200px) min(600px, 80vw), min(600px, 80vw)" srcset="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-8-data-store-components-BMATQ3EZ.png&amp;w=640&amp;q=75 640w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-8-data-store-components-BMATQ3EZ.png&amp;w=750&amp;q=75 750w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-8-data-store-components-BMATQ3EZ.png&amp;w=828&amp;q=75 828w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-8-data-store-components-BMATQ3EZ.png&amp;w=1080&amp;q=75 1080w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-8-data-store-components-BMATQ3EZ.png&amp;w=1200&amp;q=75 1200w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-8-data-store-components-BMATQ3EZ.png&amp;w=1920&amp;q=75 1920w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-8-data-store-components-BMATQ3EZ.png&amp;w=2048&amp;q=75 2048w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-8-data-store-components-BMATQ3EZ.png&amp;w=3840&amp;q=75 3840w" src="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-8-data-store-components-BMATQ3EZ.png&amp;w=3840&amp;q=75" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 8 Data store components</figcaption></div></figure>
<p><strong>Data routing service</strong></p>
<p>The data routing service provides RESTful or gRPC [12] APIs to access the data node cluster. It is a stateless service that can scale by adding more servers. This service has the following responsibilities:</p>
<ul>
<li>
<p>Query the placement service to get the best data node to store data.</p>
</li>
<li>
<p>Read data from data nodes and return it to the API service.</p>
</li>
<li>
<p>Write data to data nodes.</p>
</li>
</ul>
<p><strong>Placement service</strong></p>
<p>The placement service determines which data nodes (primary and replicas) should be chosen to store an object. It maintains a virtual cluster map, which provides the physical topology of the cluster. The virtual cluster map contains location information for each data node which the placement service uses to make sure the replicas are physically separated. This separation is key to high durability. See the “Durability” section below for details. An example of the virtual cluster map is shown in Figure 9.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a hierarchical system architecture diagram.  At the top level is a 'Root' node, branching down to a 'Default' node.  From 'Default', two branches lead to 'DC-1' and 'DC-2', representing two datacenters. Each datacenter further branches into multiple 'Host' nodes (DC-1 has Host-1 and Host-2; DC-2 has Host-3 and Host-4).  Each 'Host' node then connects to multiple 'Partition' nodes, representing data partitions residing on each host.  The connections depict a parent-child relationship, indicating a hierarchical structure where higher-level nodes manage or contain lower-level nodes.  Specifically, the 'Root' node is the top-level parent, followed by 'Default', then datacenters, hosts, and finally partitions.  The numbering of hosts and partitions suggests a sequential or logical arrangement within each level.  No URLs or parameters are present in the diagram." loading="lazy" width="700" height="254" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-9-virtual-cluster-map-ESBSZKSE.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 9 Virtual cluster map</figcaption></div></figure>
<p>The placement service continuously monitors all data nodes through heartbeats. If a data node doesn’t send a heartbeat within a configurable 15-second grace period, the placement service marks the node as “down” in the virtual cluster map.</p>
<p>This is a critical service, so we suggest building a cluster of 5 or 7 placement service nodes with Paxos [13] or Raft [14] consensus protocol. The consensus protocol ensures that as long as more than half of the nodes are healthy, the service as a whole continues to work. For example, if the placement service cluster has 7 nodes, it can tolerate a 3-node failure. To learn more about consensus protocols, refer to the reference materials [13] [14].</p>
<p><strong>Data node</strong></p>
<p>The data node stores the actual object data. It ensures reliability and durability by replicating data to multiple data nodes, also called a replication group.</p>
<p>Each data node has a data service daemon running on it. The data service daemon continuously sends heartbeats to the placement service. The heartbeat message includes the following essential information:</p>
<ul>
<li>
<p>How many disk drives (HDD or SSD) does the data node manage?</p>
</li>
<li>
<p>How much data is stored on each drive?</p>
</li>
</ul>
<p>When the placement service receives the heartbeat for the first time, it assigns an ID for this data node, adds it to the virtual cluster map, and returns the following information:</p>
<ul>
<li>
<p>a unique ID of the data node</p>
</li>
<li>
<p>the virtual cluster map</p>
</li>
<li>
<p>where to replicate data</p>
</li>
</ul>
<h4 id="data-persistence-flow">Data persistence flow</h4>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a data storage and replication system.  The system begins with an `API Service` which receives a `Write data` request (1). This request is then passed to a `Data Routing Service`. The `Data Routing Service` consults a `Placement Service` (2) to determine the primary data node for storage.  Once the primary node is selected, the `Data Routing Service` sends the data to the primary node (3). The primary node, labeled `Data Node Primary`, then replicates the data to two secondary nodes, labeled `Data Node Secondary` (4).  Heartbeats are exchanged between the primary node and each secondary node to maintain system health and monitor replication status. Finally, the `API Service` receives an `ObjId` (5) in response, indicating successful data storage.  The entire system is labeled `Data Store`, indicating the overall function of data persistence and redundancy." loading="lazy" width="700" height="315" decoding="async" data-nimg="1" sizes="(max-width: 840px) min(700px, 100vw), (max-width: 1200px) min(700px, 80vw), min(700px, 80vw)" srcset="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-10-data-persistence-flow-7TJ7KRXK.png&amp;w=640&amp;q=75 640w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-10-data-persistence-flow-7TJ7KRXK.png&amp;w=750&amp;q=75 750w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-10-data-persistence-flow-7TJ7KRXK.png&amp;w=828&amp;q=75 828w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-10-data-persistence-flow-7TJ7KRXK.png&amp;w=1080&amp;q=75 1080w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-10-data-persistence-flow-7TJ7KRXK.png&amp;w=1200&amp;q=75 1200w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-10-data-persistence-flow-7TJ7KRXK.png&amp;w=1920&amp;q=75 1920w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-10-data-persistence-flow-7TJ7KRXK.png&amp;w=2048&amp;q=75 2048w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-10-data-persistence-flow-7TJ7KRXK.png&amp;w=3840&amp;q=75 3840w" src="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-10-data-persistence-flow-7TJ7KRXK.png&amp;w=3840&amp;q=75" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 10 Data persistence flow</figcaption></div></figure>
<p>Now let’s take a look at how data is persisted in the data node.</p>
<ol>
<li>
<p>The API service forwards the object data to the data store.</p>
</li>
<li>
<p>The data routing service generates a UUID for this object and queries the placement service for the data node to store this object. The placement service checks the virtual cluster map and returns the primary data node.</p>
</li>
<li>
<p>The data routing service sends data directly to the primary data node, together with its UUID.</p>
</li>
<li>
<p>The primary data node saves the data locally and replicates it to two secondary data nodes. The primary node responds to the data routing service when data is successfully replicated to all secondary nodes.</p>
</li>
<li>
<p>The UUID of the object (ObjId) is returned to the API service.</p>
</li>
</ol>
<p>In step 2, given a UUID for the object as an input, the placement service returns the replication group for the object. How does the placement service do this? Keep in mind that this lookup needs to be deterministic, and it must survive the addition or removal of replication groups. Consistent hashing is a common implementation of such a lookup function. Refer to [15] for more information.</p>
<p>In step 4, the primary data node replicates data to all secondary nodes before it returns a response. This makes data strongly consistent among all data nodes. This consistency comes with latency costs because we have to wait until the slowest replica finishes. Figure 11 shows the trade-offs between consistency and latency.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents three different data replication strategies in a distributed system, each depicted as a timeline showing data flow and API service wait time.  Each strategy involves a `Data routing service`, a `Primary data node`, and two `Secondary data nodes`.  A thick blue line at the top of each diagram represents the `API service wait time`. Black arrows indicate data propagation between nodes.  The first option shows the data routing service waiting for data from all three nodes before responding, resulting in the longest API wait time (`Highest latency`) but ensuring the best data consistency (`Best consistency`). The second option shows the data routing service waiting for data from the primary and one secondary node, resulting in a medium API wait time (`Medium latency`) and medium consistency (`Medium consistency`). The third option shows the data routing service responding after receiving data only from the primary node, resulting in the shortest API wait time (`Lowest latency`) but the lowest data consistency (`Worst consistency`).  Each strategy's consistency and latency are labeled to the right of its corresponding diagram." loading="lazy" width="600" height="454" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-11-trade-off-between-consistency-and-latency-M5BFWVFW.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 11 Trade-off between consistency and latency</figcaption></div></figure>
<ol>
<li>
<p>Data is considered as successfully saved after all three nodes store the data. This approach has the best consistency but the highest latency.</p>
</li>
<li>
<p>Data is considered as successfully saved after the primary and one of the secondaries store the data. This approach has a medium consistency and medium latency.</p>
</li>
<li>
<p>Data is considered as successfully saved after the primary persists the data. This approach has the worst consistency but the lowest latency.</p>
</li>
</ol>
<p>Both 2 and 3 are forms of eventual consistency.</p>
<h4 id="how-data-is-organized">How data is organized</h4>
<p>Now let’s take a look at how each data node manages the data. A simple solution is to store each object in a stand-alone file. This works, but the performance suffers when there are many small files. Two issues arise when having too many small files on a file system. First, it wastes many data blocks. A file system stores files in discrete disk blocks. Disk blocks have the same size, and the size is fixed when the volume is initialized. The typical block size is around 4 KB. For a file smaller than 4 KB, it would still consume the entire disk block. If the file system holds a lot of small files, it wastes a lot of disk blocks, with each one only lightly filled with a small file.</p>
<p>Second, it could exceed the system’s inode capacity. The file system stores the location and other information about a file in a special type of block called inode. For most file systems, the number of inodes is fixed when the disk is initialized. With millions of small files, it runs the risk of consuming all inodes. Also, the operating system does not handle a large number of inodes very well, even with aggressive caching of file system metadata. For these reasons, storing small objects as individual files does not work well in practice.</p>
<p>To address these issues, we can merge many small objects into a larger file. It works conceptually like a write-ahead log (WAL). When we save an object, it is appended to an existing read-write file. When the read-write file reaches its capacity threshold – usually set to a few GBs – the read-write file is marked as read-only, and a new read-write file is created to receive new objects. Once a file is marked as read-only, it can only serve read requests. Figure 12 explains how this process works.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a system architecture diagram illustrating data interaction between an API service and a local file system.  The diagram shows an 'API Service' box at the top, from which a downward arrow points to a light pink box labeled 'object 4'. This 'object 4' represents data being written.  Below, a larger box labeled 'Local File System' contains three file representations. Two are labeled 'Read-only File' and each contains several empty horizontal rows suggesting multiple data objects. The third file is labeled 'Read-write File' and shows four horizontal rows representing data objects labeled 'object 1', 'object 2', 'object 3', and a light pink 'object 4' (mirroring the one from the API service).  A curved arrow connects the 'object 4' from the API service to the 'object 4' within the 'Read-write File', indicating that the API service is writing 'object 4' to this specific file.  The 'Read-write File' also shows 'Empty spaces' below 'object 4', suggesting available space for future data writes.  The overall diagram depicts a simple data writing process where the API service sends data ('object 4') to be stored in a specific location within the local file system's 'Read-write File'." loading="lazy" width="600" height="402" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-12-store-multiple-small-objects-in-one-big-file-VOZCHVGT.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 12 Store multiple small objects in one big file</figcaption></div></figure>
<p>Note that write access to the read-write file must be serialized. As shown in Figure 12, objects are stored in order, one after the other, in the read-write file. To maintain this on-disk layout, multiple cores processing incoming write requests in parallel must take their turns to write to the read-write file. For a modern server with a large number of cores processing many incoming requests in parallel, this seriously restricts write throughput. To fix this, we could provide dedicated read-write files, one for each core processing incoming requests.</p>
<h4 id="object-lookup">Object lookup</h4>
<p>With each data file holding many small objects, how does the data node locate an object by UUID? The data node needs the following information:</p>
<ul>
<li>
<p>The data file that contains the object</p>
</li>
<li>
<p>The starting offset of the object in the data file</p>
</li>
<li>
<p>The size of the object</p>
</li>
</ul>
<p>The database schema to support this lookup is shown in Table 3.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a table schema named 'object_mapping'.  The table has four columns:  `object_id` (presumably a unique identifier for each object), `file_name` (a string representing the name of the file where the object is stored), `start_offset` (a numerical value indicating the starting position of the object within the file), and `object_size` (a numerical value representing the size of the object in bytes).  No data rows are shown; only the column names and their data types are depicted. The table's purpose is to map objects to their corresponding file locations and sizes, facilitating efficient retrieval of objects from storage." loading="lazy" width="335" height="245" decoding="async" data-nimg="1" sizes="(max-width: 840px) min(335px, 100vw), (max-width: 1200px) min(335px, 80vw), min(335px, 80vw)" srcset="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ftable-3-object_mapping-table-N2BUA76T.png&amp;w=640&amp;q=75 640w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ftable-3-object_mapping-table-N2BUA76T.png&amp;w=750&amp;q=75 750w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ftable-3-object_mapping-table-N2BUA76T.png&amp;w=828&amp;q=75 828w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ftable-3-object_mapping-table-N2BUA76T.png&amp;w=1080&amp;q=75 1080w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ftable-3-object_mapping-table-N2BUA76T.png&amp;w=1200&amp;q=75 1200w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ftable-3-object_mapping-table-N2BUA76T.png&amp;w=1920&amp;q=75 1920w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ftable-3-object_mapping-table-N2BUA76T.png&amp;w=2048&amp;q=75 2048w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ftable-3-object_mapping-table-N2BUA76T.png&amp;w=3840&amp;q=75 3840w" src="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ftable-3-object_mapping-table-N2BUA76T.png&amp;w=3840&amp;q=75" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Table 3 Object_mapping table</figcaption></div></figure>
<table><thead><tr><td><strong>Field</strong></td><td><strong>Description</strong></td></tr></thead><tbody><tr><td>object_id</td><td>UUID of the object</td></tr><tr><td>file_name</td><td>The name of the file that contains the object</td></tr><tr><td>start_offset</td><td>Beginning address of the object in the file</td></tr><tr><td>object_size</td><td>The number of bytes in the object</td></tr></tbody></table>
<p class="tableCaption">Table 4 Object_mapping fields</p>
<p>We considered two options for storing this mapping: a file-based key-value store such as RocksDB [16] or a relational database. RocksDB is based on SSTable [17], and it is fast for writes but slower for reads. A relational database usually uses a B+ tree [18] based storage engine, and it is fast for reads but slower for writes. As mentioned earlier, the data access pattern is write once and read multiple times. Since a relational database provides better read performance, it is a better choice than RocksDB.</p>
<p>How should we deploy this relational database? At our scale, the data volume for the mapping table is massive. Deploying a single large cluster to support all data nodes could work, but is difficult to manage. Note that this mapping data is isolated within each data node. There is no need to share this across data nodes. To take advantage of this property, we could simply deploy a simple relational database on each data node. SQLite [19] is a good choice here. It is a file-based relational database with a solid reputation.</p>
<h4 id="updated-data-persistence-flow">Updated data persistence flow</h4>
<p>Since we have made quite a few changes to the data node, let’s revisit how to save a new object in the data node (Figure 13).</p>
<ol>
<li>
<p>The API service sends a request to save a new object named “object 4”.</p>
</li>
<li>
<p>The data node service appends the object named “object 4” at the end of the read-write file named “/data/c”.</p>
</li>
<li>
<p>A new record of “object 4” is inserted into the <em>object_mapping</em> table.</p>
</li>
<li>
<p>The data node service returns the UUID to the API service.</p>
</li>
</ol>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a system architecture diagram illustrating data management and interaction between a local machine and a remote service.  The local machine contains a local file system with three files: `/data/a` and `/data/b` (both read-only), and `/data/c` (read-write).  The `/data/c` file is depicted as containing several objects (object 1, 2, 3, and 4, with object 4 highlighted).  An API Service interacts with a Data Node Service, which in turn interacts with an Object Mapping Database.  The Data Node Service receives requests (numbered 1-4) from the API Service.  Request 2 and 3 are shown as connecting to the Object Mapping Database, a relational database with a table named `object_mapping table` containing columns for `obj_id`, `file_name`, `offset`, and `obj_size`.  Dashed lines indicate data flow: the `file_name`, `start_offset`, and `object_size` of objects in `/data/c` are used to query the `object_mapping table` in the database, which returns the `obj_id` (e.g., object 6) and its corresponding metadata.  The database entry for object 6 shows its file name as `/data/c`, its offset as `0x232B3`, and its size as `512`.  The overall diagram shows how the system manages and accesses data, mapping objects within files on the local machine to entries in the remote database." loading="lazy" width="700" height="392" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-13-updated-data-persistence-flow-WV6KYPEI.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 13 Updated data persistence flow</figcaption></div></figure>
<h4 id="durability">Durability</h4>
<p>Data reliability is extremely important for data storage systems. How can we create a storage system that offers six nines of durability? Each failure case has to be carefully considered and data needs to be properly replicated.</p>
<h5 id="hardware-failure-and-failure-domain">Hardware failure and failure domain</h5>
<p>Hard drive failures are inevitable no matter which media we use. Some storage media may have better durability than others, but we cannot rely on a single hard drive to achieve our durability objective. A proven way to increase durability is to replicate data to multiple hard drives, so a single disk failure does not impact the data availability, as a whole. In our design, we replicate data three times.</p>
<p>Let’s assume the spinning hard drive has an annual failure rate of 0.81% [20]. This number highly depends on the model and make. Making 3 copies of data gives us 1-(0.0081)^3=~0.999999 reliability. This is a very rough estimate. For more sophisticated calculations, please read [20].</p>
<p>For a complete durability evaluation, we also need to consider the impacts of different failure domains. A failure domain is a physical or logical section of the environment that is negatively affected when a critical service experiences problems. In a modern data center, a server is usually put into a rack [21], and the racks are grouped into rows/floors/rooms. Since each rack shares network switches and power, all the servers in a rack are in a rack-level failure domain. A modern server shares components like the motherboard, processors, power supply, HDD drives, etc. The components in a server are in a node-level failure domain.</p>
<p>Here is a good example of a large-scale failure domain isolation. Typically, data centers divide infrastructure that shares nothing into different Availability Zones (AZs). We replicate our data to different AZs to minimize the failure impact (Figure 14). Note that the choice of failure domain level doesn’t directly increase the durability of data, but it will result in better reliability in extreme cases, such as large-scale power outages, cooling system failures, natural disasters, etc.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a three-Availability Zone (AZ) system design for data replication.  AZ 1 contains two racks, each with two nodes labeled 'Node 1' and 'Node 2'.  Similarly, AZ 2 and AZ 3 each have two racks, each with two nodes identically labeled.  AZ 1 acts as the primary source, with data replicating ('Replicate' label on arrows) to both AZ 2 and AZ 3.  AZ 2 and AZ 3 also replicate data to each other, ensuring redundancy and high availability. Each AZ is described as having '(Independent power and networking),' indicating that they are physically separate and independent, providing fault tolerance.  The overall architecture demonstrates a multi-AZ replication strategy for data redundancy and disaster recovery." loading="lazy" width="600" height="443" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-14-multi-datacenter-replication-NTRDXDU5.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 14 Multi-datacenter replication</figcaption></div></figure>
<h5 id="erasure-coding">Erasure coding</h5>
<p>Making three full copies of data gives us roughly 6 nines of data durability. Are there other options to further increase durability? Yes, erasure coding is one option. Erasure coding [22] deals with data durability differently. It chunks data into smaller pieces (placed on different servers) and creates parities for redundancy. In the event of failures, we can use chunk data and parities to reconstruct the data. Let’s take a look at a concrete example (4 + 2 erasure coding) as shown in Figure 15.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a four-stage process illustrating data redundancy and reconstruction using parity.  Stage 1 ('Split into equal-sized data chunks') shows a data source (a rectangle) distributing four equal-sized data chunks (d1, d2, d3, d4) to subsequent stages. Stage 2 ('Calculate parities') depicts the generation of two parity chunks (p1 and p2, shown in light pink) from the data chunks.  Each data chunk contributes to the calculation of both parity chunks, represented by lines connecting each 'd' to both 'p' boxes. Stage 3 ('Data loss due to node crash') simulates a node failure, indicated by 'x' markings on d3 and d4, representing their loss.  Finally, Stage 4 ('Data reconstruction') shows how the remaining data chunks (d1, d2, p1, p2) are used to reconstruct the lost data chunks (d3 and d4), with lines indicating the data flow used for this reconstruction.  The overall diagram demonstrates a system's resilience to data loss through the use of parity checks." loading="lazy" width="500" height="346" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-15-erasure-coding-WXEDUOMP.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 15 Erasure coding</figcaption></div></figure>
<ol>
<li>
<p>Data is broken up into four even-sized data chunks d1, d2, d3, and d4.</p>
</li>
<li>
<p>The mathematical formula [23] is used to calculate the parities p1 and p2. To give a much simplified example, p1 = d1 + 2*d2 - d3 + 4*d4 and p2 = -d1 + 5*d2 + d3 - 3*d4 [24].</p>
</li>
<li>
<p>Data d3 and d4 are lost due to node crashes.</p>
</li>
<li>
<p>The mathematical formula is used to reconstruct lost data d3 and d4, using the known values of d1, d2, p1, and p2.</p>
</li>
</ol>
<p>Let’s take a look at another example as shown in Figure 16 to better understand how erasure coding works with failure domains. An (8+4) erasure coding setup breaks up the original data evenly into 8 chunks and calculates 4 parities. All 12 pieces of data have the same size. All 12 chunks of data are distributed across 12 different failure domains. The mathematics behind erasure coding ensures that the original data can be reconstructed when at most 4 nodes are down.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a system design for a 'Math Calculation' process.  At the top, a rectangular box labeled 'Math Calculation' receives input from a horizontal row of twelve smaller boxes, each containing a small diamond or circle (presumably representing data points or tasks).  These twelve boxes are connected to twelve larger square boxes arranged in a three-by-four grid below, each labeled 'Failure Domain.'  Each 'Failure Domain' box contains a 5x5 grid of smaller squares, with one square in each 'Failure Domain' box highlighted with either a purple diamond or a blue circle, indicating a specific data point or task assigned to that domain.  Lines connect the top twelve boxes to the highlighted squares in the 'Failure Domain' boxes, illustrating the distribution of the calculation workload across these domains. The arrangement suggests a distributed system where the main calculation is broken down into smaller, independent tasks handled by separate failure domains, allowing for fault tolerance and scalability." loading="lazy" width="500" height="611" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-16-8-4-erasure-coding-SPQ6N2I3.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 16 (8+4) erasure coding</figcaption></div></figure>
<p>Compared to replication where the data router only needs to read data for an object from one healthy node, in erasure coding the data router has to read data from at least 8 healthy nodes. This is an architectural design tradeoff. We use a more complex solution with a slower access speed, in exchange for higher durability and lower storage cost. For object storage where the main cost is storage, this tradeoff might be worth it.</p>
<p>How much extra space does erasure coding need? For every two chunks of data, we need one parity block, so the storage overhead is 50% (Figure 17). While in 3-copy replication, the storage overhead is 200% (Figure 17).</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="The image represents a comparison of two data distribution strategies: 3-copy replication and erasure coding (4+2).  The top section illustrates 3-copy replication, showing a 3GB dataset (1GB per node) distributed across three nodes.  Each node contains an identical 1GB copy of the data, indicated by three equally sized, light pink rectangles labeled '1GB'.  An arrow points from the text '3-copy replication' and 'Data is distributed across 3 nodes' to these rectangles. The bottom section depicts erasure coding (4+2), where the same 3GB dataset is divided into six 0.25GB chunks. These chunks are distributed across six nodes, represented by six smaller, equally sized, light pink rectangles labeled '0.25 GB'.  Arrows point from the text 'Data is distributed across 6 nodes' and 'Erasure coding (4+2)' to these smaller rectangles.  The visual difference highlights the redundancy and distribution differences between the two approaches; 3-copy replication uses more storage but offers simpler data retrieval, while erasure coding uses less storage but requires more complex data reconstruction in case of node failures." loading="lazy" width="300" height="306" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-17-extra-space-for-replication-and-erasure-coding-JXD3Y3GX.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 17 Extra space for replication and erasure coding</figcaption></div></figure>
<p>Does erasure coding increase data durability? Let’s assume a node has a 0.81% annual failure rate. According to the calculation done by Backblaze [20], erasure coding can achieve 11 nines durability. The calculation requires complicated math. If you’re interested, refer to [20] for details.</p>
<p>Table 5 compares the pros and cons of replication and erasure coding.</p>
<div class="table-wrap" style="--table-min-width: 640px;"><table><thead><tr><td><strong></strong></td><td><strong>Replication</strong></td><td><strong>Erasure coding</strong></td></tr></thead><tbody><tr><td>durability</td><td>6 nines of durability (data copied 3 times)</td><td><p>11 nines of durability (8+4 erasure coding). <strong>Erasure coding wins</strong>.</p></td></tr><tr><td>Storage efficiency </td><td>200% storage overhead.</td><td>50% storage overhead. <strong>Erasure coding wins</strong>.</td></tr><tr><td>Compute resource </td><td>No computation. <strong>Replication wins.</strong></td><td>Higher usage of computation resources to calculate parities.</td></tr><tr><td>Write performance </td><td><p>Replicating data to multiple nodes. No calculation is needed.
<strong>Replication wins.</strong></p></td><td><p>Increased write latency because we need to calculate parities before
data is written to disk.</p></td></tr><tr><td>Read performance</td><td><p>In normal operation, reads are served from the same replica. Reads
under a failure mode are not impacted because reads can be served from
a non-fault replica. <strong>Replication wins.</strong> </p></td><td><p>In normal operation, every read has to come from multiple nodes in the
cluster. Reads under a failure mode are slower because the missing
data must be reconstructed first.</p></td></tr></tbody></table></div>
<p class="tableCaption">Table 5 Replication vs erasure coding</p>
<p>In summary, replication is widely adopted in latency-sensitive applications while erasure coding is often used to minimize storage cost. Erasure coding is attractive for its cost efficiency and durability, but it greatly complicates the data node design. Therefore, for this design, we mainly focus on replication.</p>
<h4 id="correctness-verification">Correctness verification</h4>
<p>Erasure coding increases data durability at comparable storage costs. Now we can move on to solve another hard challenge: data corruption.</p>
<p>If a disk fails completely and the failure can be detected, it can be treated as a data node failure. In this case, we can reconstruct data using erasure coding. However, in-memory data corruption is a regular occurrence in large-scale systems.</p>
<p>This problem can be addressed by verifying checksums [25] between process boundaries. A checksum is a small-sized block of data that is used to detect data errors. Figure 18 illustrates how the checksum is generated.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a data flow diagram illustrating the process of checksum generation.  The diagram begins with a labeled box titled 'Data' containing three lines of binary data: '011010101010110', '1010010101001', and '01000100010100'.  An arrow points from this data box to a rectangular box labeled 'Checksum algorithm'. This box represents the computational process where the input binary data is processed using a checksum algorithm.  Another arrow extends from the 'Checksum algorithm' box to a final box labeled 'Checksum', which contains the resulting hexadecimal checksum 'F7 33 51 1B 9D F6'. The arrows indicate the unidirectional flow of data: the binary data is input into the algorithm, and the resulting checksum is the output." loading="lazy" width="600" height="103" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-18-generate-checksum-TOHMO3NC.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 18 Generate checksum</figcaption></div></figure>
<p>If we know the checksum of the original data, we can compute the checksum of the data after transmission:</p>
<ul>
<li>
<p>If they are different, data is corrupted.</p>
</li>
<li>
<p>If they are the same, there is a very high probability the data is not corrupted. The probability is not 100%, but in practice, we could assume they are the same.</p>
</li>
</ul>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a data integrity check using checksums.  Two rectangular boxes are shown, the left one labeled 'Checksum of the original data' containing the hexadecimal value 'F7 33 51 1B 9D F6', and the right one labeled 'Checksum of the received data' containing the identical hexadecimal value 'F7 33 51 1B 9D F6'. An arrow labeled 'Compare' connects the two boxes, indicating that the checksum of the original data is compared to the checksum of the received data.  The implication is that if the checksums match, as they do in this image, the data has been transmitted without error." loading="lazy" width="500" height="84" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-19-compare-checksums-CQGRDK6J.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 19 Compare checksums</figcaption></div></figure>
<p>There are many checksum algorithms, such as MD5 [26], SHA1[27], HMAC [28], etc. A good checksum algorithm usually outputs a significantly different value even for a small change made to the input. For this chapter, we choose a simple checksum algorithm such as MD5.</p>
<p>In our design, we append the checksum at the end of each object. Before a file is marked as read-only, we add a checksum of the entire file at the end. Figure 20 shows the layout.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a simplified illustration of a local file system showing three files. Two files are labeled 'Read-only File' and are depicted as rectangular boxes divided into several horizontal sections representing data blocks; each file concludes with a pink-shaded section labeled 'Checksum'.  The third file, labeled 'Read-write File,' is also a rectangular box, but it's structured differently. It contains five data objects (object 1 through object 5), each paired with a corresponding 'Checksum' in a pink-shaded adjacent column.  Below these objects, there's a section labeled 'Empty spaces,' indicating unused space within the file. The entire diagram is labeled 'Local File System' at the bottom, suggesting the representation is of how files are stored and organized on a local storage device.  No information flow is explicitly shown; the diagram focuses on the structure and organization of data and checksums within the files." loading="lazy" width="500" height="476" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-20-add-checksum-to-data-node-ICOVBL6G.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 20 Add checksum to data node</figcaption></div></figure>
<p>With (8+4) erasure coding and checksum verification, this is what happens when we read data:</p>
<ol>
<li>
<p>Fetch the object data and the checksum.</p>
</li>
<li>
<p>Compute the checksum against the data received.</p>
<p>(a). If the two checksums match, the data is error-free.</p>
<p>(b). If the checksums are different, the data is corrupted. We will try to recover by reading the data from other failure domains.</p>
</li>
<li>
<p>Repeat steps 1 and 2 until all 8 pieces of data are returned. We then reconstruct the data and send it back to the client.</p>
</li>
</ol>
<h3 id="metadata-data-model">Metadata data model</h3>
<p>In this section, we first discuss the database schema and then dive into scaling the database.</p>
<h5 id="schema">Schema</h5>
<p>The database schema needs to support the following 3 queries:</p>
<p>Query 1: Find the object ID by object name.</p>
<p>Query 2: Insert and delete an object based on the object name.</p>
<p>Query 3: List objects in a bucket sharing the same prefix.</p>
<p>Figure 21 shows the schema design. We need two database tables: bucket and object.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents two database tables depicted as simple rectangular boxes. The left table is labeled 'bucket' and contains four fields: 'bucket_name', 'bucket_id', 'owner_id', and 'enable_versioning'.  The right table is labeled 'object' and contains five fields: 'bucket_name', 'object_name', 'object_version', and 'object_id'.  Each field is listed vertically within its respective table.  There is no visual connection or information flow explicitly shown between the two tables; they are presented independently, suggesting a relational database structure where the 'object' table likely has a foreign key relationship with the 'bucket' table via the 'bucket_name' field, implying that objects belong to buckets." loading="lazy" width="400" height="145" decoding="async" data-nimg="1" sizes="(max-width: 840px) min(400px, 100vw), (max-width: 1200px) min(400px, 80vw), min(400px, 80vw)" srcset="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-21-database-tables-DAGCX3WM.png&amp;w=640&amp;q=75 640w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-21-database-tables-DAGCX3WM.png&amp;w=750&amp;q=75 750w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-21-database-tables-DAGCX3WM.png&amp;w=828&amp;q=75 828w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-21-database-tables-DAGCX3WM.png&amp;w=1080&amp;q=75 1080w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-21-database-tables-DAGCX3WM.png&amp;w=1200&amp;q=75 1200w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-21-database-tables-DAGCX3WM.png&amp;w=1920&amp;q=75 1920w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-21-database-tables-DAGCX3WM.png&amp;w=2048&amp;q=75 2048w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-21-database-tables-DAGCX3WM.png&amp;w=3840&amp;q=75 3840w" src="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Ffigure-21-database-tables-DAGCX3WM.png&amp;w=3840&amp;q=75" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 21 Database tables</figcaption></div></figure>
<h5 id="scale-the-bucket-table">Scale the bucket table</h5>
<p>Since there is usually a limit on the number of buckets a user can create, the size of the bucket table is small. Let’s assume we have 1 million customers, each customer owns 10 buckets and each record takes 1 KB. That means we need 10 GB (1 million * 10 * 1KB) of storage space. The whole table can easily fit in a modern database server. However, a single database server might not have enough CPU or network bandwidth to handle all read requests. If so, we can spread the read load among multiple database replicas.</p>
<h5 id="scale-the-object-table">Scale the object table</h5>
<p>The object table holds the object metadata. The dataset at our design scale will likely not fit in a single database instance. We can scale the object table by sharding.</p>
<p>One option is to shard by the <em>bucket_id</em> so all the objects under the same bucket are stored in one shard. This doesn’t work because it causes hotspot shards as a bucket might contain billions of objects.</p>
<p>Another option is to shard by <em>object_id</em>. The benefit of this sharding scheme is that it evenly distributes the load. But we will not be able to execute query 1 and query 2 efficiently because those two queries are based on the URI.</p>
<p>We choose to shard by a combination of <em>bucket_name</em> and <em>object_name</em>. This is because most of the metadata operations are based on the object URI, for example, finding the object ID by URI or uploading an object via URI. To evenly distribute the data, we can use the hash of the (<em>bucket_name</em>, <em>object_name)</em> as the sharding key.</p>
<p>With this sharding scheme, it is straightforward to support the first two queries, but the last query is less obvious. Let’s take a look.</p>
<h3 id=""></h3>
<h3 id="listing-objects-in-a-bucket">Listing objects in a bucket</h3>
<p>The object store arranges files in a flat structure instead of a hierarchy, like in a file system. An object can be accessed using a path in this format, <em>s3://bucket-name/object-name.</em> For example, <em>s3://mybucket/abc/d/e/f/file.txt</em> contains:</p>
<ul>
<li>
<p>Bucket name: mybucket</p>
</li>
<li>
<p>Object name: abc/d/e/f/file.txt</p>
</li>
</ul>
<p>To help users organize their objects in a bucket, S3 introduces a concept called ‘prefixes.’ A prefix is a string at the beginning of the object name. S3 uses prefixes to organize the data in a way similar to directories. However, prefixes are not directories. Listing a bucket by prefix limits the results to only those object names that begin with the prefix.</p>
<p>In the example above with the path <em>s3://mybucket/abc/d/e/f/file.txt</em>, the prefix is <em>abc/d/e/f/.</em></p>
<p>The AWS S3 listing command has 3 typical uses:</p>
<ol>
<li>
<p>List all buckets owned by a user. The command looks like this:</p>
<pre><code>aws s3 list-buckets
</code></pre>
</li>
<li>
<p>List all objects in a bucket that are at the same level as the specified prefix. The command looks like this:</p>
<pre><code>aws s3 ls s3://mybucket/abc/
</code></pre>
<p>In this mode, objects with more slashes in the name after the prefix are rolled up into a common prefix. For example, with these objects in the bucket:</p>
<pre><code>CA/cities/losangeles.txt
CA/cities/sanfrancisco.txt
NY/cities/ny.txt
federal.txt
</code></pre>
<p>Listing the bucket with the ‘/’ prefix would return these results, with everything under CA/ and NY/ rolled up into them:</p>
<p>CA/ NY/ federal.txt</p>
</li>
<li>
<p>Recursively list all objects in a bucket that shares the same prefix. The command looks like this:</p>
<pre><code>aws s3 ls s3://mybucket/abc/ --recursive
</code></pre>
<p>Using the same example as above, listing the bucket with the <code>CA/</code> prefix would return these results:</p>
<pre><code>CA/cities/losangeles.txt
CA/cities/sanfrancisco.txt
</code></pre>
</li>
</ol>
<h4 id="single-database">Single database</h4>
<p>Let’s first explore how we would support the listing command with a single database. To list all buckets owned by a user, we run the following query:</p>
<pre><code>SELECT * FROM bucket WHERE owner_id={id}
</code></pre>
<p>To list all objects in a bucket that share the same prefix, we run a query like this.</p>
<pre><code>SELECT * FROM object WHERE bucket_id = "123" AND object_name LIKE `abc/%`
</code></pre>
<p>In this example, we find all objects with bucket_id equals to “123” that share the prefix “abc/”. Any objects with more slashes in their names after the specified prefix are rolled up in the application code as stated earlier in use case 2.</p>
<p>The same query would support the recursive listing mode, as stated in use case 3 previously. The application code would list every object sharing the same prefix, without performing any rollups.</p>
<h4 id="distributed-databases">Distributed databases</h4>
<p>When the metadata table is sharded, it’s difficult to implement the listing function because we don’t know which shards contain the data. The most obvious solution is to run a search query on all shards and then aggregate the results. To achieve this, we can do the following:</p>
<ol>
<li>
<p>The metadata service queries every shard by running the following query:</p>
<pre><code>SELECT * FROM object WHERE bucket_id = “123” AND object_name LIKE `a/b/%`
</code></pre>
</li>
<li>
<p>The metadata service aggregates all objects returned from each shard and returns the result to the caller.</p>
</li>
</ol>
<p>This solution works, but implementing pagination for this is a bit complicated. Before we explain why, let’s review how pagination works for a simple case with a single database. To return pages of listing with 10 objects for each page, the SELECT query would start with this:</p>
<pre><code>SELECT * FROM object WHERE
bucket_id = "123" AND object_name LIKE `a/b/%`
ORDER BY object_name OFFSET 0 LIMIT 10
</code></pre>
<p>The OFFSET and LIMIT would restrict the results to the first 10 objects. In the next call, the user sends the request with a hint to the server, so it knows to construct the query for the second page with an OFFSET of 10. This hint is usually done with a cursor that the server returns with each page to the client. The offset information is encoded in the cursor. The client would include the cursor in the request for the next page. The server decodes the cursor and uses the offset information embedded in it to construct the query for the next page. To continue with the example above, the query for the second page looks like this:</p>
<pre><code>SELECT * FROM metadata
WHERE bucket_id = "123" AND object_name LIKE `a/b/%`
ORDER BY object_name OFFSET 10 LIMIT 10
</code></pre>
<p>This client-server request loop continues until the server returns a special cursor that marks the end of the entire listing.</p>
<p>Now, let’s explore why it’s complicated to support pagination for sharded databases. Since the objects are distributed across shards, the shards would likely return a varying number of results. Some shards would contain a full page of 10 objects, while others would be partial or empty. The application code would receive results from every shard, aggregate and sort them, and return only a page of 10 in our example. The objects that don’t get included in the current round must be considered again for the next round. This means that each shard would likely have a different offset. The server must track the offsets for all the shards and associate those offsets with the cursor. If there are hundreds of shards, there will be hundreds of offsets to track.</p>
<p>We have a solution that can solve the problem, but there are some tradeoffs. Since object storage is tuned for vast scale and high durability, object listing performance is rarely a priority. In fact, all commercial object storage supports object listing with sub-optimal performance. To take advantage of this, we could denormalize the listing data into a separate table sharded by bucket ID. This table is only used for listing objects. With this setup, even buckets with billions of objects would offer acceptable performance. This isolates the listing query to a single database which greatly simplifies the implementation.</p>
<h3 id="object-versioning">Object versioning</h3>
<p>Versioning is a feature that keeps multiple versions of an object in a bucket. With versioning, we can restore objects that are accidentally deleted or overwritten. For example, we may modify a document and save it under the same name, inside the same bucket. Without versioning, the old version of the document metadata is replaced by the new version in the metadata store. The old version of the document is marked as deleted, so its storage space will be reclaimed by the garbage collector. With versioning, the object storage keeps all previous versions of the document in the metadata store, and the old versions of the document are never marked as deleted in the object store.</p>
<p>Figure 22 explains how to upload a versioned object. For this to work, we first need to enable versioning on the bucket.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a system architecture diagram for object storage.  A user initiates an HTTP PUT object request (①), which is first handled by a load balancer distributing the request to an API Service.  The API Service then interacts with an Identity Service (②) for user authentication and authorization.  After successful authentication, the API Service checks (④) if the object exists and if versioning is enabled.  Next, the object is uploaded (③) to a Storage Service, which is replicated across a primary and two secondary Storage Nodes forming the Data Store.  Concurrently, the API Service creates object metadata (⑤) and stores it in a Metadata Service, which persists the data in a Metadata DB.  The Metadata Service is responsible for managing object metadata, while the Storage Services handle the actual object storage and replication for high availability.  The entire system is designed for robust object storage with features like authentication, versioning, and data redundancy." loading="lazy" width="700" height="496" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-22-object-versioning-RHMOOBIT.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 22 Object versioning</figcaption></div></figure>
<ol>
<li>
<p>The client sends an HTTP PUT request to upload an object named “script.txt”.</p>
</li>
<li>
<p>The API service verifies the user’s identity and ensures that the user has WRITE permission on the bucket.</p>
</li>
<li>
<p>Once verified, the API service uploads the data to the data store. The data store persists the data as a new object and returns a new UUID to the API service.</p>
</li>
<li>
<p>The API service calls the metadata store to store the metadata information of this object.</p>
</li>
<li>
<p>To support versioning, the object table for the metadata store has a column called object_version that is only used if versioning is enabled. Instead of overwriting the existing record, a new record is inserted with the same bucket_id and object_name as the old record, but with a new object_id and object_version. The object_id is the UUID for the new object returned in step 3. The object_version is a TIMEUUID [29] generated when the new row is inserted. No matter which database we choose for the metadata store, it should be efficient to look up the current version of an object. The current version has the largest TIMEUUID of all the entries with the same object_name. See Figure 23 for an illustration of how we store versioned metadata.</p>
</li>
</ol>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a system for versioning data, showing a 'Metadata Store' and a 'Data Store'.  The Metadata Store contains three entries, each representing a version of a data object named 'script.txt'. Each entry includes an `object_version` (e.g., 'fas3', 'bn31', '1ag1') and an `object_id` (e.g., '0xM13n', '0x12Ha', '0x91b4').  Dashed arrows indicate that the `object_version` entries in the Metadata Store are linked to previous versions, with 'fas3' representing the current version and '1ag1' the oldest.  Solid arrows show that each `object_version` entry points to a corresponding entry in the Data Store, which contains the actual data object identified by its `object_id`.  The Data Store shows three data objects, each represented by a rectangle connected to its corresponding `object_id` in the Metadata Store.  The overall structure illustrates how metadata tracks different versions of data objects, allowing retrieval of specific versions based on their version identifier." loading="lazy" width="700" height="291" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-23-versioned-metadata-RKJ5JZER.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 23 Versioned metadata</figcaption></div></figure>
<p>In addition to uploading a versioned object, it can also be deleted. Let’s take a look.</p>
<p>When we delete an object, all versions remain in the bucket and we insert a delete marker, as shown in Figure 24.</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a system for managing object versions, showing a 'Metadata Store' and a 'Data Store'.  The Metadata Store contains entries for each object version, including the `object_version` (e.g., `kk1h`, `fas3`, `bn31`, `1ag1`), `object_name` (always 'script.txt'), and `object_id` (e.g., `0xM13n`, `0x12Ha`, `0x91b4`).  The current version (`object_version = kk1h`) is explicitly labeled, and a previous current version (`object_version = fas3`) is also indicated.  A pink-filled box labeled 'Delete Marker' is associated with the current version in the Metadata Store. Dashed arrows indicate the relationship between the previous current versions and their entries in the Metadata Store. Solid arrows show the connection between the `object_version` in the left column and the corresponding entry in the Metadata Store.  The Data Store contains entries linked to the `object_id` from the Metadata Store, each connected by a dashed arrow.  Each `object_id` in the Data Store corresponds to a specific object version, and each entry in the Data Store is represented by a simple box, suggesting the actual object data is stored there.  The overall structure illustrates how metadata tracks object versions and their corresponding data locations within the Data Store." loading="lazy" width="700" height="339" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-24-delete-object-by-inserting-a-delete-marker-NTTDXHAH.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 24 Delete object by inserting a delete marker</figcaption></div></figure>
<p>A delete marker is a new version of the object, and it becomes the current version of the object once inserted. Performing a GET request when the current version of the object is a delete marker returns a 404 Object Not Found error.</p>
<h3 id="-1"></h3>
<h3 id="optimizing-uploads-of-large-files">Optimizing uploads of large files</h3>
<p>In the back-of-the-envelope estimation, we estimated that 20% of the objects are large. Some might be larger than a few GBs. It is possible to upload such a large object file directly, but it could take a long time. If the network connection fails in the middle of the upload, we have to start over. A better solution is to slice a large object into smaller parts and upload them independently. After all the parts are uploaded, the object store re-assembles the object from the parts. This process is called multipart upload.</p>
<p>Figure 25 illustrates how multipart upload works:</p>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a sequence diagram illustrating a multipart file upload process.  The diagram shows a user (represented by a stick figure) interacting with a 'Data Store' (a rectangular box). The process is divided into three phases: Initiation, Multipart Upload, and Completion.  Initiation begins with the user initiating a multipart upload (step ①), receiving a unique `uploadID` (step ②) in return from the Data Store.  The Multipart Upload phase involves sending individual file parts (Part 1, Part 2...Part 8) to the Data Store, each accompanied by the `uploadID`.  For each part uploaded (step ③), the Data Store responds with an `ETag` (step ④), a unique identifier for that part.  Finally, in the Completion phase, the user sends a multipart upload completion message (step ⑤) to the Data Store, including the `uploadID` and all the previously obtained `ETags` (ETag 1, ETag 2...ETag 8). Upon successful completion, the Data Store sends a 'Success' message (step ⑥) to the user.  The dashed lines delineate the phases, and numbered circles indicate the sequential steps in the process.  Arrows show the direction of information flow between the user and the Data Store." loading="lazy" width="500" height="619" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-25-multipart-upload-KDHGYB2U.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 25 Multipart upload</figcaption></div></figure>
<ol>
<li>
<p>The client calls the object storage to initiate a multipart upload.</p>
</li>
<li>
<p>The data store returns an uploadID, which uniquely identifies the upload.</p>
</li>
<li>
<p>The client splits the large file into small objects and starts uploading. Let’s assume the size of the file is 1.6GB and the client splits it into 8 parts, so each part is 200 MB in size. The client uploads the first part to the data store together with the uploadID it received in step 2.</p>
</li>
<li>
<p>When a part is uploaded, the data store returns an ETag, which is essentially the md5 checksum of that part. It is used to verify multipart uploads.</p>
</li>
<li>
<p>After all parts are uploaded, the client sends a complete multipart upload request, which includes the uploadID, part numbers, and ETags.</p>
</li>
<li>
<p>The data store reassembles the object from its parts based on the part number. Since the object is really large, this process may take a few minutes. After reassembly is complete, it returns a success message to the client.</p>
</li>
</ol>
<p>One potential problem with this approach is that old parts are no longer useful after the object has been reassembled from them. To solve this problem, we can introduce a garbage collection service responsible for freeing up space from parts that are no longer needed.</p>
<h3 id="garbage-collection">Garbage collection</h3>
<p>Garbage collection is the process of automatically reclaiming storage space that is no longer used. There are a few ways that data might become garbage:</p>
<ul>
<li>
<p>Lazy object deletion. An object is marked as deleted at delete time without actually being deleted.</p>
</li>
<li>
<p>Orphan data. For example, half uploaded data or abandoned multipart uploads.</p>
</li>
<li>
<p>Corrupted data. Data that failed the checksum verification.</p>
</li>
</ul>
<p>The garbage collector does not remove objects from the data store, right away. Deleted objects will be periodically cleaned up with a compaction mechanism.</p>
<p>The garbage collector is also responsible for reclaiming unused space in replicas. For replication, we delete the object from both primary and backup nodes. For erasure coding, if we use (8+4) setup, we delete the object from all 12 nodes.</p>
<p>Figure 26 shows an example of how compaction works.</p>
<ol>
<li>
<p>The garbage collector copies objects from “/data/b” to a new file named “/data/d”. Note the garbage collector skips “Object 2” and “Object 5” because the delete flag is set to true for both of them.</p>
</li>
<li>
<p>After all objects are copied, the garbage collector updates the object_mapping table. For example, the obj_id and object_size fields of “Object 3” remain the same, but file_name and start_offset are updated to reflect its new location. To ensure data consistency, it’s a good idea to wrap the update operations to file_name and start_offset in a database transaction.</p>
</li>
</ol>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a data compaction process.  The diagram shows two states: 'Before Compaction' and 'After Compaction,' each with a read-only file and an `object_mapping` table. The table has columns `obj_id`, `file_name`, `offset`, and `obj_size`. Before compaction, the read-only file contains Object 2, Object 3, and Object 5, sequentially.  Object 3, identified by `obj_id` 'object 3' and `file_name` '/data/b', is located at offset `0x232B3` in the file, as indicated in the `object_mapping` table.  After compaction, Object 2 and Object 5 are removed, and Object 3 is moved to a new location.  The `object_mapping` table is updated to reflect this change; Object 3 now has `file_name` '/data/d' and offset `0x10013`.  Dashed arrows show the movement of Object 3 (labeled '1') from its old location to its new, compacted location.  Numbered curved arrows ('2') illustrate the update process in the `object_mapping` table, showing the change in file name and offset for Object 3.  The 'X' markings highlight the objects removed during compaction." loading="lazy" width="700" height="587" decoding="async" data-nimg="1" src="https://bytebytego.com/images/courses/system-design-interview/s3-like-object-storage/figure-26-compaction-GGRMBRFH.svg" style="color: transparent;"></div><div style="display: flex; justify-content: center;"><figcaption class="py-1">Figure 26 Compaction</figcaption></div></figure>
<p>As we can see from Figure 26, the size of the new file after compaction is smaller than the old file. To avoid creating a lot of small files, the garbage collector usually waits until there are a large number of read-only files to compact, and the compaction process appends objects from many read-only files into a few large new files.</p>
<h2 id="step-4---wrap-up">Step 4 - Wrap Up</h2>
<p>In this chapter, we described the high-level design for S3-like object storage. We compared the differences between block storage, file storage, and object storage.</p>
<p>The focus of this interview is on the design of object storage, so we listed how the uploading, downloading, listing objects in a bucket, and versioning of objects are typically done in object storage.</p>
<p>Then we dived deeper into the design. Object storage is composed of a data store and a metadata store. We explained how the data is persisted into the data store and discussed two methods for increasing reliability and durability: replication and erasure coding. For the metadata store, we explained how the multipart upload is executed and how to design the database schema to support typical use cases. Lastly, we explained how to shard the metadata store to support even larger data volume.</p>
<p>Congratulations on getting this far! Now give yourself a pat on the back. Good job!</p>
<h2 id="-2"></h2>
<h2 id="chapter-summary">Chapter Summary</h2>
<figure class="py-1"><div style="display: flex; justify-content: center;"><img alt="Image represents a mind map outlining the design process for an S3-like object storage system.  The central node is 'S3-like Object Storage,' branching into four numbered steps. Step 1 details 'functional requirements,' including bucket creation, object uploading/downloading, object versioning, and listing objects within a bucket.  It also lists 'non-functional requirements,' specifying 100PB data capacity, 6 nines of durability, and 4 nines of availability, along with storage efficiency. Step 2 focuses on the 'high-level design' of uploading and downloading objects. Step 3 delves into the 'data store,' encompassing data persistence flow, how data is organized, durability (achieved through replication and erasure coding), and correctness verification.  This step also addresses the 'metadata data model,' including schema design and scaling strategies for both bucket and object tables, along with object versioning, optimization for large file uploads, and garbage collection. Finally, step 4 simply notes 'wrap up.'  The connections between nodes illustrate the hierarchical breakdown of the design process, from high-level requirements to low-level implementation details." loading="lazy" width="650" height="826" decoding="async" data-nimg="1" sizes="(max-width: 840px) min(650px, 100vw), (max-width: 1200px) min(650px, 80vw), min(650px, 80vw)" srcset="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Fchapter-summary-CM6JBEGR.png&amp;w=640&amp;q=75 640w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Fchapter-summary-CM6JBEGR.png&amp;w=750&amp;q=75 750w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Fchapter-summary-CM6JBEGR.png&amp;w=828&amp;q=75 828w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Fchapter-summary-CM6JBEGR.png&amp;w=1080&amp;q=75 1080w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Fchapter-summary-CM6JBEGR.png&amp;w=1200&amp;q=75 1200w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Fchapter-summary-CM6JBEGR.png&amp;w=1920&amp;q=75 1920w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Fchapter-summary-CM6JBEGR.png&amp;w=2048&amp;q=75 2048w, https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Fchapter-summary-CM6JBEGR.png&amp;w=3840&amp;q=75 3840w" src="https://bytebytego.com/_next/image?url=%2Fimages%2Fcourses%2Fsystem-design-interview%2Fs3-like-object-storage%2Fchapter-summary-CM6JBEGR.png&amp;w=3840&amp;q=75" style="color: transparent;"></div></figure>
<h2 id="-3"></h2>
<h2 id="reference-material">Reference Material</h2>
<p>[1] Fibre channel: <a href="https://en.wikipedia.org/wiki/Fibre_Channel" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/Fibre_Channel</u></a></p>
<p>[2] iSCSI: <a href="https://en.wikipedia.org/wiki/ISCSI" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/ISCSI</u></a></p>
<p>[3] Server Message Block: <a href="https://en.wikipedia.org/wiki/Server_Message_Block" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/Server_Message_Block</u></a></p>
<p>[4] Network File System: <a href="https://en.wikipedia.org/wiki/Network_File_System" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/Network_File_System</u></a></p>
<p>[5] Amazon S3 Strong Consistency: <a href="https://aws.amazon.com/s3/consistency/" target="_blank" rel="noopener noreferrer"><u>https://aws.amazon.com/s3/consistency/</u></a></p>
<p>[6] Serial Attached SCSI: <a href="https://en.wikipedia.org/wiki/Serial_Attached_SCSI" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/Serial_Attached_SCSI</u></a></p>
<p>[7] AWS CLI ls command: <a href="https://docs.aws.amazon.com/cli/latest/reference/s3/ls.html" target="_blank" rel="noopener noreferrer"><u>https://docs.aws.amazon.com/cli/latest/reference/s3/ls.html</u></a></p>
<p>[8] Amazon S3 Service Level Agreement: <a href="https://aws.amazon.com/s3/sla/" target="_blank" rel="noopener noreferrer"><u>https://aws.amazon.com/s3/sla/</u></a></p>
<p>[9] Ambry: LinkedIn’s Scalable Geo-Distributed Object Store:<br>
<a href="https://assured-cloud-computing.illinois.edu/files/2014/03/Ambry-LinkedIns-Scalable-GeoDistributed-Object-Store.pdf" target="_blank" rel="noopener noreferrer"><u>https://assured-cloud-computing.illinois.edu/files/2014/03/Ambry-LinkedIns-Scalable-GeoDistributed-Object-Store.pdf</u></a></p>
<p>[10] inode: <a href="https://en.wikipedia.org/wiki/Inode" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/Inode</u></a></p>
<p>[11] Ceph’s Rados Gateway: <a href="https://docs.ceph.com/en/pacific/radosgw/index.html" target="_blank" rel="noopener noreferrer"><u>https://docs.ceph.com/en/pacific/radosgw/index.html</u></a></p>
<p>[12] grpc: <a href="https://grpc.io/" target="_blank" rel="noopener noreferrer"><u>https://grpc.io/</u></a></p>
<p>[13] Paxos: <a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/Paxos_(computer_science)</u></a></p>
<p>[14] Raft: <a href="https://raft.github.io/" target="_blank" rel="noopener noreferrer"><u>https://raft.github.io/</u></a></p>
<p>[15] Consistent hashing: <a href="https://www.toptal.com/big-data/consistent-hashing" target="_blank" rel="noopener noreferrer"><u>https://www.toptal.com/big-data/consistent-hashing</u></a></p>
<p>[16] RocksDB: <a href="https://github.com/facebook/rocksdb" target="_blank" rel="noopener noreferrer"><u>https://github.com/facebook/rocksdb</u></a></p>
<p>[17] SSTable: <a href="https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/" target="_blank" rel="noopener noreferrer"><u>https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/</u></a></p>
<p>[18] B+ tree: <a href="https://en.wikipedia.org/wiki/B%2B_tree" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/B%2B_tree</u></a></p>
<p>[19] SQLite: <a href="https://www.sqlite.org/index.html" target="_blank" rel="noopener noreferrer"><u>https://www.sqlite.org/index.html</u></a></p>
<p>[20] Data Durability Calculation: <a href="https://www.backblaze.com/blog/cloud-storage-durability/" target="_blank" rel="noopener noreferrer"><u>https://www.backblaze.com/blog/cloud-storage-durability/</u></a></p>
<p>[21] Rack: <a href="https://en.wikipedia.org/wiki/19-inch_rack" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/19-inch_rack</u></a></p>
<p>[22] Erasure Coding: <a href="https://en.wikipedia.org/wiki/Erasure_code" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/Erasure_code</u></a></p>
<p>[23] Reed–Solomon error correction: <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction</u></a></p>
<p>[24] Erasure Coding Demystified: <a href="https://www.youtube.com/watch?v=Q5kVuM7zEUI" target="_blank" rel="noopener noreferrer"><u>https://www.youtube.com/watch?v=Q5kVuM7zEUI</u></a></p>
<p>[25] Checksum:<a href="https://en.wikipedia.org/wiki/Checksum" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/Checksum</u></a></p>
<p>[26] Md5: <a href="https://en.wikipedia.org/wiki/MD5" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/MD5</u></a></p>
<p>[27] Sha1: <a href="https://en.wikipedia.org/wiki/SHA-1" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/SHA-1</u></a></p>
<p>[28] Hmac: <a href="https://en.wikipedia.org/wiki/HMAC" target="_blank" rel="noopener noreferrer"><u>https://en.wikipedia.org/wiki/HMAC</u></a></p>
<p>[29] TIMEUUID: <a href="https://docs.datastax.com/en/cql-oss/3.3/cql/cql_reference/timeuuid_functions_r.html" target="_blank" rel="noopener noreferrer"><u>https://docs.datastax.com/en/cql-oss/3.3/cql/cql_reference/timeuuid_functions_r.html</u></a></p>
        </article>
    </main>

    <div class="nav">
        <a href="../system-design-interview.html">← Course Contents</a>
        <a href="distributed-email-service.html">← Previous</a>
        <a href="real-time-gaming-leaderboard.html">Next →</a>
    </div>

    <footer class="metadata">
        <p>Scraped on 10/10/2025</p>
    </footer>
</body>
</html>